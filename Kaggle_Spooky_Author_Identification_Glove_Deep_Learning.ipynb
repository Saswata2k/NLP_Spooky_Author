{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:42:45.521946Z",
     "start_time": "2019-02-17T20:42:42.570404Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from glove import Corpus, Glove\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import datetime,time\n",
    "from flashtext.keyword import KeywordProcessor\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from flashtext.keyword import KeywordProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# libraries for dataset preparation, feature engineering, model training \n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "#from utils.language_models import WordVectorEmbedding\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "#import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import spooky author data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:42:47.004370Z",
     "start_time": "2019-02-17T20:42:46.947905Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv',encoding='utf-8')\n",
    "df.head()\n",
    "sen=df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T20:21:03.575071Z",
     "start_time": "2019-02-12T20:21:03.570253Z"
    }
   },
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:43:47.243392Z",
     "start_time": "2019-02-17T20:43:47.238218Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function for removing regex\n",
    "def regex_filtering(text):\n",
    "        if text:\n",
    "            #removing all email metadata fix it for email terms only\n",
    "            text=re.sub(r\"^(sender|to|copy|from|sent|subject|date|cc|e|von|datum|an|importance|bcc):.*$\",\" \",text,flags=re.M)\n",
    "            #removing all mail ids\n",
    "            text=re.sub(r\"\\S*@\\S*\\s?\",\" \",text)\n",
    "            #removing all links\n",
    "            text=re.sub(r\"(((https?|ftp|file):\\/\\/)|www\\\\.)\\\\S+\", ' ', text, flags=re.MULTILINE)\n",
    "            text=re.sub(r\"\\w*\\.\\w{1,4}\", '', text, flags=re.MULTILINE)\n",
    "            #removing all non word character\n",
    "            text=re.sub(r\"([^a-zA-Z0-9\\\\u00C0-\\\\u00FF@]|[Ã£Ã¢])+\",' ',text)\n",
    "            #removing words with numbers \n",
    "            text=re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "            #removing single characters\n",
    "            text=re.sub(r'\\b\\S{1}\\s+',' ',text)\n",
    "            #removing words with repeating characters\n",
    "            text=re.sub(r'\\b(\\w)\\1{1,}\\s+',' ',text)\n",
    "            #removing punkt\n",
    "            text = text.translate(str.maketrans('','',string.punctuation))\n",
    "            #removing extra whitespace\n",
    "            text=re.sub(r\"\\s\\s+\",' ',text)\n",
    "            #removing repeating words\n",
    "            text=re.sub(r\"(\\w+\\s+)\\1{1,}\",' ',text)\n",
    "            #removing whitespaces\n",
    "            text=text.strip()\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:43:48.069838Z",
     "start_time": "2019-02-17T20:43:48.066040Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize Terms and remove stopwords\n",
    "def tokenize_term(x):\n",
    "        predefined_stopwords='horror perfectly'\n",
    "        english_stopwords=stopwords.words(\"english\")\n",
    "        stopwords_list=(list(predefined_stopwords.split(' '))+english_stopwords)         \n",
    "        keyword_processor_stopwords = KeywordProcessor()\n",
    "        for each in stopwords_list:\n",
    "            keyword_processor_stopwords.add_keyword(each,' ')   \n",
    "        sentence=keyword_processor_stopwords.replace_keywords(x)\n",
    "        return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:43:49.302456Z",
     "start_time": "2019-02-17T20:43:49.299190Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combined function\n",
    "def preprocess(text):\n",
    "    return regex_filtering(tokenize_term(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:43:21.708468Z",
     "start_time": "2019-02-17T20:43:11.611739Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply function preprocess and save result as new column\n",
    "df['text_preprocessed']=df['text'].apply(preprocess)\n",
    "texts=df['text_preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:45:13.400874Z",
     "start_time": "2019-02-17T20:45:13.390979Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>process however afforded means ascertaining di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>never occurred fumbling might mere mistake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>left hand gold snuff box capered hill cutting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>lovely spring looked Windsor Terrace sixteen f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>Finding nothing else even gold Superintendent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                   text_preprocessed  \n",
       "0  process however afforded means ascertaining di...  \n",
       "1         never occurred fumbling might mere mistake  \n",
       "2  left hand gold snuff box capered hill cutting ...  \n",
       "3  lovely spring looked Windsor Terrace sixteen f...  \n",
       "4  Finding nothing else even gold Superintendent ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:47:46.974337Z",
     "start_time": "2019-02-17T20:47:41.945445Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>process however afforded means ascertaining di...</td>\n",
       "      <td>process however afford mean ascertain dimensio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>never occurred fumbling might mere mistake</td>\n",
       "      <td>never occur fumble might mere mistake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>left hand gold snuff box capered hill cutting ...</td>\n",
       "      <td>leave hand gold snuff box caper hill cut manne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>lovely spring looked Windsor Terrace sixteen f...</td>\n",
       "      <td>lovely spring look Windsor Terrace sixteen fer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>Finding nothing else even gold Superintendent ...</td>\n",
       "      <td>Finding nothing else even gold Superintendent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                   text_preprocessed  \\\n",
       "0  process however afforded means ascertaining di...   \n",
       "1         never occurred fumbling might mere mistake   \n",
       "2  left hand gold snuff box capered hill cutting ...   \n",
       "3  lovely spring looked Windsor Terrace sixteen f...   \n",
       "4  Finding nothing else even gold Superintendent ...   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  process however afford mean ascertain dimensio...  \n",
       "1              never occur fumble might mere mistake  \n",
       "2  leave hand gold snuff box caper hill cut manne...  \n",
       "3  lovely spring look Windsor Terrace sixteen fer...  \n",
       "4  Finding nothing else even gold Superintendent ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatize different forms of words\n",
    "def lemmatize_doc(dataframe):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer() \n",
    "    lemmatized = [[wordnet_lemmatizer.lemmatize(word,pos='v') for word in word_tokenize(s)]\n",
    "                  for s in dataframe['text_preprocessed']]\n",
    "    return lemmatized\n",
    "lemma_data=lemmatize_doc(df)\n",
    "df['text_lemmatized']=[\" \".join(i) for i in lemma_data]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:48:58.993887Z",
     "start_time": "2019-02-17T20:48:58.989211Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/100D_spooky_author_glove_embedding_model_2019-03-02_50_epoch.model\n",
      "model/100D_spooky_author_glove_embedding_model_2019-03-02_50_epoch.txt\n"
     ]
    }
   ],
   "source": [
    "###Saving models with timestamp\n",
    "import datetime,time\n",
    "\n",
    "path=\"model/\"\n",
    "epochs=50\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d')+\"_\"\n",
    "model_file_path =  path + \"100D_spooky_author_glove_embedding_model_\"+str(st) + str(epochs) + \"_epoch.model\"\n",
    "glove_vectors_path = path + \"100D_spooky_author_glove_embedding_model_\" + str(st)+ str(epochs) + \"_epoch.txt\"\n",
    "#glove_to_w2v_path = path + \"spooky_author_glove_embedding_model_\" +str(st)  + \"_glove_to_word2vec.txt\"\n",
    "\n",
    "print (model_file_path)\n",
    "print (glove_vectors_path)\n",
    "#print(glove_to_w2v_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyper-parameters for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:49:04.106805Z",
     "start_time": "2019-02-17T20:49:04.103321Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Parameters for glove model\n",
    "window = 5         ##### Contect window of every sentence it has to take\n",
    "no_components=100  ##### No.of Dimensions or size of vectors\n",
    "#no_components_100=100  ##### No.of Dimensions or size of vectors\n",
    "learning_rate=0.05 ##### Learning rate or alpha\n",
    "no_threads=84      ##### Multiprocessing ( specify the cores)\n",
    "epochs=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train glove embedding and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:50:30.720512Z",
     "start_time": "2019-02-17T20:50:24.579493Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a corpus object\n",
    "corpus = Corpus() \n",
    "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus.fit(lemma_data, window=window)\n",
    "\n",
    "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "#We can set the learning rate as it uses Gradient Descent and number of components\n",
    "glove = Glove(no_components=no_components, learning_rate=learning_rate)\n",
    " \n",
    "glove.fit(corpus.matrix, epochs=epochs, no_threads=no_threads, verbose=False)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove embedding model trained successfully. \n",
      "Model saved in the following path : model/100D_spooky_author_glove_embedding_model_2019-03-02_50_epoch.model\n"
     ]
    }
   ],
   "source": [
    "glove.save(model_file_path)\n",
    "print('Glove embedding model trained successfully. \\nModel saved in the following path : {}'.format(model_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model in vector format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:53:21.390052Z",
     "start_time": "2019-02-17T20:53:20.429568Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model in vector format\n",
    "word_vectors = glove.word_vectors\n",
    "\n",
    "# Writing a vector file \n",
    "#Sample word : word1 0.2178995609092781 -0.3523739278544217 -0.14607002613618192 -0.0019162911041566608 \n",
    "\n",
    "words = list(glove.dictionary)\n",
    "word_vectors_for_words  = word_vectors.tolist()\n",
    "glove_vectors = [m + \" \" + str(\" \".join(str(item) for item in n)) for m,n in zip(words,word_vectors_for_words)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:53:23.731833Z",
     "start_time": "2019-02-17T20:53:23.675085Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(glove_vectors_path,\"w+\") as fp:\n",
    "    for line in glove_vectors:\n",
    "        fp.write(str(line))\n",
    "        fp.write(\"\\n\")\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:54:38.288213Z",
     "start_time": "2019-02-17T20:54:38.277792Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>process however afforded means ascertaining di...</td>\n",
       "      <td>process however afford mean ascertain dimensio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>never occurred fumbling might mere mistake</td>\n",
       "      <td>never occur fumble might mere mistake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>left hand gold snuff box capered hill cutting ...</td>\n",
       "      <td>leave hand gold snuff box caper hill cut manne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>lovely spring looked Windsor Terrace sixteen f...</td>\n",
       "      <td>lovely spring look Windsor Terrace sixteen fer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>Finding nothing else even gold Superintendent ...</td>\n",
       "      <td>Finding nothing else even gold Superintendent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                   text_preprocessed  \\\n",
       "0  process however afforded means ascertaining di...   \n",
       "1         never occurred fumbling might mere mistake   \n",
       "2  left hand gold snuff box capered hill cutting ...   \n",
       "3  lovely spring looked Windsor Terrace sixteen f...   \n",
       "4  Finding nothing else even gold Superintendent ...   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  process however afford mean ascertain dimensio...  \n",
       "1              never occur fumble might mere mistake  \n",
       "2  leave hand gold snuff box caper hill cut manne...  \n",
       "3  lovely spring look Windsor Terrace sixteen fer...  \n",
       "4  Finding nothing else even gold Superintendent ...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove null rows present after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:54:42.485904Z",
     "start_time": "2019-02-17T20:54:42.476502Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_=df['text_lemmatized'].apply(lambda x:True if not x else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:54:43.513750Z",
     "start_time": "2019-02-17T20:54:43.494256Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty strings 0\n",
      "After removing empty strings 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Before removing empty strings {}\".format(filter_.sum()))\n",
    "df=df[~filter_]\n",
    "filter_=df['text_lemmatized'].apply(lambda x:True if not x else False)\n",
    "print(\"After removing empty strings {}\".format(filter_.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:55:28.670558Z",
     "start_time": "2019-02-17T20:55:28.662762Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df[~filter_]\n",
    "df.to_pickle('df_authors_lemmatized_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find average vector of each sentence from the trained glove vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:56:31.811739Z",
     "start_time": "2019-02-17T20:56:31.807367Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['text_lemmatized'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text_lemmatized'])\n",
    "data = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:56:44.327737Z",
     "start_time": "2019-02-17T20:56:34.649072Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19573, 100)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build neural network with LSTM\n",
    "#### Network Architechture\n",
    "The network starts with an embedding layer. The layer lets the system expand each token to a more massive vector, allowing the network to represent a word in a meaningful way. The layer takes 20000 as the first argument, which is the size of our vocabulary, and 100 as the second input parameter, which is the dimension of the embeddings. The third parameter is the input_length of 50, which is the length of each comment sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:57:42.766320Z",
     "start_time": "2019-02-17T20:57:39.158757Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(20000, 100, input_length=100))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(3, activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:57:52.989220Z",
     "start_time": "2019-02-17T20:57:52.966450Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=df['author']\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels=le.fit_transform(df['author'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14679 samples, validate on 4894 samples\n",
      "Epoch 1/10\n",
      "14679/14679 [==============================] - 73s 5ms/step - loss: 0.2094 - acc: 0.9278 - val_loss: 0.5588 - val_acc: 0.8081\n",
      "Epoch 2/10\n",
      "14679/14679 [==============================] - 69s 5ms/step - loss: 0.1093 - acc: 0.9611 - val_loss: 0.6540 - val_acc: 0.7949\n",
      "Epoch 3/10\n",
      "14679/14679 [==============================] - 78s 5ms/step - loss: 0.0758 - acc: 0.9725 - val_loss: 0.8168 - val_acc: 0.7983\n",
      "Epoch 4/10\n",
      "14679/14679 [==============================] - 77s 5ms/step - loss: 0.0624 - acc: 0.9769 - val_loss: 0.8540 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "14679/14679 [==============================] - 72s 5ms/step - loss: 0.0429 - acc: 0.9855 - val_loss: 1.0004 - val_acc: 0.7850\n",
      "Epoch 6/10\n",
      "14679/14679 [==============================] - 68s 5ms/step - loss: 0.0412 - acc: 0.9853 - val_loss: 1.0108 - val_acc: 0.7906\n",
      "Epoch 7/10\n",
      "14679/14679 [==============================] - 67s 5ms/step - loss: 0.0312 - acc: 0.9897 - val_loss: 1.1257 - val_acc: 0.7897\n",
      "Epoch 8/10\n",
      "14679/14679 [==============================] - 67s 5ms/step - loss: 0.0265 - acc: 0.9911 - val_loss: 1.1996 - val_acc: 0.7857\n",
      "Epoch 9/10\n",
      "14679/14679 [==============================] - 67s 5ms/step - loss: 0.0223 - acc: 0.9924 - val_loss: 1.1632 - val_acc: 0.7748\n",
      "Epoch 10/10\n",
      "14679/14679 [==============================] - 68s 5ms/step - loss: 0.0206 - acc: 0.9932 - val_loss: 1.3587 - val_acc: 0.7850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d621d8b5f8>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels_cat = to_categorical(labels)\n",
    "model_lstm.fit(data, np.array(labels_cat), validation_split=0.25, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Label-encoder to transform author class labels to machine understandable number inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:59:29.676464Z",
     "start_time": "2019-02-17T20:59:29.651290Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Using pretrained word vector\n",
    "pretrained_vector_path='glove.6B.100d.txt'\n",
    "embeddings_index = dict()\n",
    "f = open(pretrained_vector_path,encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T20:59:46.606735Z",
     "start_time": "2019-02-17T20:59:46.574391Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  same model architecture with a convolutional layer on top of the LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T21:02:13.028871Z",
     "start_time": "2019-02-17T21:02:13.026404Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "vocabulary_size=20000\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 100, input_length=100, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(3))\n",
    "model_glove.add(Activation('softmax'))\n",
    "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14679 samples, validate on 4894 samples\n",
      "Epoch 1/10\n",
      "14679/14679 [==============================] - 28s 2ms/step - loss: 0.8991 - acc: 0.5764 - val_loss: 0.8092 - val_acc: 0.6300\n",
      "Epoch 2/10\n",
      "14679/14679 [==============================] - 24s 2ms/step - loss: 0.7895 - acc: 0.6529 - val_loss: 0.7679 - val_acc: 0.6671\n",
      "Epoch 3/10\n",
      "14679/14679 [==============================] - 26s 2ms/step - loss: 0.7211 - acc: 0.6861 - val_loss: 0.7649 - val_acc: 0.6633\n",
      "Epoch 4/10\n",
      "14679/14679 [==============================] - 27s 2ms/step - loss: 0.6611 - acc: 0.7178 - val_loss: 0.7333 - val_acc: 0.6819\n",
      "Epoch 5/10\n",
      "14679/14679 [==============================] - 26s 2ms/step - loss: 0.6083 - acc: 0.7406 - val_loss: 0.7522 - val_acc: 0.6708\n",
      "Epoch 6/10\n",
      "14679/14679 [==============================] - 24s 2ms/step - loss: 0.5603 - acc: 0.7650 - val_loss: 0.7622 - val_acc: 0.6810\n",
      "Epoch 7/10\n",
      "14679/14679 [==============================] - 31s 2ms/step - loss: 0.5287 - acc: 0.7766 - val_loss: 0.7629 - val_acc: 0.6786\n",
      "Epoch 8/10\n",
      "14679/14679 [==============================] - 30s 2ms/step - loss: 0.4950 - acc: 0.7930 - val_loss: 0.7617 - val_acc: 0.6825\n",
      "Epoch 9/10\n",
      "14679/14679 [==============================] - 26s 2ms/step - loss: 0.4576 - acc: 0.8130 - val_loss: 0.7832 - val_acc: 0.6835\n",
      "Epoch 10/10\n",
      "14679/14679 [==============================] - 26s 2ms/step - loss: 0.4442 - acc: 0.8159 - val_loss: 0.7926 - val_acc: 0.6839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d632614208>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels_cat = to_categorical(labels)\n",
    "model_glove.fit(data, np.array(labels_cat), validation_split=0.25, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
